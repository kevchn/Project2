{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (After Finals):\n",
    "# - Add learning rate schedule\n",
    "# - Add momentum schedule\n",
    "# - Try different momentum (Nesterov, etc)\n",
    "# - Benchmark GPU times, AWS p3.2xlarge spot instance\n",
    "# - Try out superconvergence (Smith & Topin)\n",
    "# - Bayesian optimization hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='# of batches to cycle before logging training status')\n",
    "args = parser.parse_args(\"--epochs=3 --log-interval=100 --no-cuda\".split())  # parsing is for .py, kept here with manual parse_args for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Setup\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1110a3370>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Seed\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Deviation of MNIST\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root='../data', train=True, download=True, transform=train_transform)\n",
    "MNIST_mean = train_set.train_data.float().mean()/255\n",
    "MNIST_std = train_set.train_data.float().std()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MNIST is an unusual image dataset (gray scale with virtually only two colors: black and white), we examine the mean and standard deviation of the dataset, which turn out to be 0.1307 and 0.3081, respectively. We will use these to normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((MNIST_mean,), (MNIST_std,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((MNIST_mean,), (MNIST_std,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Pytorch's DataLoader functionality to split the MNIST dataset to training and test splits automatically (from their raw partitions), convert them into tensors, and then normalize them. We also choose to shuffle the training dataset. This helps reduce variance and addresses the nonconvexity of the loss landscape by (hypothetically) avoiding local minimizers that don't generalize well. Also, by shuffling the data that is put into our minibatches, we decrease the likelihood that our gradients derived from our minibatches are unrepresentative of the true gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Simple Convolutional Neural Net (inspired by LeNet) \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 120)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model that was heavily inspired by LeNet-5, we have 3 convolutional layers which are each followed by a maxpool and relu (except for the last one which doesn't have a maxpool), and then one final fully connected layer and a relu activation before the final output layer, with a softmax activation. We also drew inspiration and guidance from the \"Convnet Architectures\" section of Stanford's course on Convolutional Neural Networks for Computer Vision \"http://cs231n.github.io/convolutional-networks/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SGD as our optimizer with fixed learning rate and momentum. Dynamic learning rate and momentum schedules have been shown to help improve generalization performance in certain scenarios, but these are probably overkill for something like MNIST. Nevertheless, it is something interesting to try in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # zero out the gradients (they accumulate)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Epoch: {} {}/{} ({:.0f}%)\\tLoss: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As briefly mentioned above, we use minibatches to take advantage of the efficiency of SGD while not sacrificing the generalization performance of full gradient descent. For each training batch, we compute the loss (we use negative log-likelihood for simplicity, since we covered this in class), compute the gradients through backprop, and then allow SGD to adjust the weights of our network accordingly. When we run through all examples in our training set, we finish what we call one epoch, and we start a new epoch again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest: Average loss: {:.3f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply . The code is very straightforward and is almost identical to the approach taken in the PyTorch documentation/tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 0/60000 (0%)\tLoss: 0.404\n",
      "Epoch: 1 6400/60000 (11%)\tLoss: 0.364\n",
      "Epoch: 1 12800/60000 (21%)\tLoss: 0.478\n",
      "Epoch: 1 19200/60000 (32%)\tLoss: 0.130\n",
      "Epoch: 1 25600/60000 (43%)\tLoss: 0.421\n",
      "Epoch: 1 32000/60000 (53%)\tLoss: 0.445\n",
      "Epoch: 1 38400/60000 (64%)\tLoss: 0.377\n",
      "Epoch: 1 44800/60000 (75%)\tLoss: 0.184\n",
      "Epoch: 1 51200/60000 (85%)\tLoss: 0.244\n",
      "Epoch: 1 57600/60000 (96%)\tLoss: 0.325\n",
      "\n",
      "Test: Average loss: 0.266, Accuracy: 9152/10000 (92%)\n",
      "\n",
      "Epoch: 2 0/60000 (0%)\tLoss: 0.480\n",
      "Epoch: 2 6400/60000 (11%)\tLoss: 0.341\n",
      "Epoch: 2 12800/60000 (21%)\tLoss: 0.203\n",
      "Epoch: 2 19200/60000 (32%)\tLoss: 0.223\n",
      "Epoch: 2 25600/60000 (43%)\tLoss: 0.100\n",
      "Epoch: 2 32000/60000 (53%)\tLoss: 0.191\n",
      "Epoch: 2 38400/60000 (64%)\tLoss: 0.265\n",
      "Epoch: 2 44800/60000 (75%)\tLoss: 0.235\n",
      "Epoch: 2 51200/60000 (85%)\tLoss: 0.230\n",
      "Epoch: 2 57600/60000 (96%)\tLoss: 0.098\n",
      "\n",
      "Test: Average loss: 0.191, Accuracy: 9413/10000 (94%)\n",
      "\n",
      "Epoch: 3 0/60000 (0%)\tLoss: 0.105\n",
      "Epoch: 3 6400/60000 (11%)\tLoss: 0.080\n",
      "Epoch: 3 12800/60000 (21%)\tLoss: 0.381\n",
      "Epoch: 3 19200/60000 (32%)\tLoss: 0.059\n",
      "Epoch: 3 25600/60000 (43%)\tLoss: 0.111\n",
      "Epoch: 3 32000/60000 (53%)\tLoss: 0.226\n",
      "Epoch: 3 38400/60000 (64%)\tLoss: 0.076\n",
      "Epoch: 3 44800/60000 (75%)\tLoss: 0.182\n",
      "Epoch: 3 51200/60000 (85%)\tLoss: 0.294\n",
      "Epoch: 3 57600/60000 (96%)\tLoss: 0.093\n",
      "\n",
      "Test: Average loss: 0.306, Accuracy: 8996/10000 (90%)\n",
      "\n",
      "Epoch: 4 0/60000 (0%)\tLoss: 0.344\n",
      "Epoch: 4 6400/60000 (11%)\tLoss: 0.163\n",
      "Epoch: 4 12800/60000 (21%)\tLoss: 0.084\n",
      "Epoch: 4 19200/60000 (32%)\tLoss: 0.121\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
